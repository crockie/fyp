\chapter{Detailed Implementation} \label{chapter:detailed_implementation}
This chapter provides an in-depth explanation of the design, architecture, and deployment of the newly implemented ASR system. It elaborates on the functionality of the system components and how they interact to achieve scalability, fault tolerance, and efficient processing.

\section{Architecture}
The new architecture of the ASR system is illustrated in Figure \ref{fig:new_architecture}. The system is designed with a microservices architecture, where each component is responsible for a specific task. 

The system consists of the following components:
\begin{itemize}
    \item \textbf{Client:} The frontend user interface that initiates transcription requests and receives the results.
    \item \textbf{NGINX Ingress Controller:} Routes incoming requests to the Live Processing Service.
    \item \textbf{Live Processing Service:} Formerly known as the Master Pod, it manages transcription tasks, coordinates worker allocation with the Worker Manager, and maintains a WebSocket connection with the client.
    \item \textbf{Worker Manager:} Allocates workers to process audio data based on the requested model and monitors the worker pods.
    \item \textbf{Worker:} Process the audio data using the specified ASR model and return the transcription results.
    \item \textbf{Message Queues:} RabbitMQ is used to decouple the communication between the Live Processing Service and the Worker.
    \item \textbf{Redis:} Stores the state information of the system, such as worker statuses and task details.
\end{itemize}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/new_architecture.drawio.png}
    \caption{New Architecture of the ASR System}
    \label{fig:new_architecture}
\end{figure}

\subsection{System Flow}
The sequence diagram in Figure \ref{fig:sequence_diagram} provides a detailed visualization of the ASR system's flow, showing the interactions between components during the transcription process.

\clearpage

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/sequence_diagram.drawio.png}
    \caption{Sequence Diagram of the ASR System}
    \label{fig:sequence_diagram}
\end{figure}

\subsubsection{Main Flow}
\begin{enumerate}
    \item The client establishes a WebSocket connection through the NGINX Ingress Controller to the Live Processing Service.
    \item Upon receiving the connection, the Live Processing Service creates two queues in RabbitMQ for each task:
    \begin{itemize}
        \item \texttt{task.<UUID of the task>}: Queue for the audio data to be processed.
        \item \texttt{result.<UUID of the task>}: Queue for the transcription results of the task.
    \end{itemize} 
    \item The Live Processing Service invokes the Worker Managerâ€™s gRPC API to allocate a worker for the task.
    \item The Live Processing Service places audio data into the task queue.
    \item The Worker Manager checks the Redis database for an idle worker with the requested ASR model and assigns it to the task. The worker is notified via a gRPC call.
    \item The worker updates its state in Redis to \texttt{BUSY} and begins processing the audio data.
    \item Partial transcription results are sent to the result queue as the audio is processed.
    \item The Live Processing Service retrieves these partial results from the result queue and streams them back to the client via the WebSocket connection.
    \item Workers send periodic heartbeats to the Worker Manager to confirm their health status.
    \item When the client detects the end of speech, it notifies the Live Processing Service, which marks the task as complete by sending a special End of Speech (\texttt{EOS}) message to the task queue.
    \item Upon processing the \texttt{EOS} message, the worker updates its state in Redis to \texttt{IDLE} and sends the final transcription result to the result queue.
    \item The Live Processing Service retrieves the final result from the result queue and delivers it to the client via WebSocket.
    \item Once all results are delivered, the WebSocket connection is closed.
\end{enumerate}

\subsubsection{Worker Failure Flow}
To maintain fault tolerance, the system detects and recovers from worker failures as follows:
\begin{enumerate}
    \item The Worker Manager detects a failure when it stops receiving heartbeats from a worker.
    \item It gets a new idle worker with the same ASR model from Redis and reassigns the task to the new worker.
    \item The new worker retrieves the pending task from the task queue, updates its state to \texttt{BUSY}, and resumes processing the audio data.
\end{enumerate}

\subsubsection{Live Processing Service Server Failure Flow}
To handle failures of the Live Processing Service:
\begin{enumerate}
    \item The client reestablishes a WebSocket connection to the Live Processing Service and resends the transcription request using the same task UUID.
    \item The Live Processing Service uses the existing task and result queues to continue processing and retrieving results, ensuring no data is lost.
\end{enumerate}

\section{Live Processing Service}
The Live Processing Service manages WebSocket connections with clients to facilitate real-time transcription. Its primary functions include:
\begin{enumerate}
  \item \textbf{WebSocket Communication:} Establishing and managing WebSocket connections to stream transcription results in real time.
  \item \textbf{Task Management:} Creating and managing task and result queues in RabbitMQ for each transcription request.
  \item \textbf{Worker Allocation:} Requesting worker allocation from the Worker Manager based on the required ASR model.
\end{enumerate}


The Live Processing Service is built using Tornado \cite{tornado}, an asynchronous web framework optimized for handling long-lived connections and scaling to thousands of concurrent WebSocket sessions. This makes it well-suited for real-time applications such the ASR system.

\subsection{WebSocket Handler}
The WebSocket handler manages WebSocket connections with clients and streams transcription results back to them. It processes the following events:

\begin{itemize}
    \item \textbf{open:} Authenticates incoming WebSocket requests, initializes task and result queues in RabbitMQ, requests worker allocation from the Worker Manager, and starts consuming transcription results from the result queue.
    \item \textbf{on\_message:} Receives audio data from the client and places it into the task queue.
    \item \textbf{close:} Closes the WebSocket connection with the client and performs cleanup operations.
\end{itemize}

\subsection{Asynchronous RabbitMQ}
To maintain non-blocking performance, the Live Processing Service utilizes an asynchronous RabbitMQ client compatible with Tornado. This enables concurrent task management without disrupting the event loop.

A dedicated asynchronous RabbitMQ client with exponential backoff was implemented to handle network disruptions and RabbitMQ failures. Code \ref{lst:rabbitmq_client} illustrates the implementation.

\begin{lstlisting}[language=python, caption={Asynchronous RabbitMQ Client}, label={lst:rabbitmq_client}]
async def connect(self):
      """
      Connect to RabbitMQ.
      """
      attempts = 0
      while attempts < RABBITMQ_MAX_RETRIES:
          try:
              self.logger.info(f"trying to connect to {self.host}:{self.port}")
              self.logger.info(f"using {self.username} as username")
              self.connection = await aio_pika.connect_robust(
                  host=self.host,
                  port=self.port,
                  login=self.username,
                  password=self.password,
              )
              self.logger.info("Connected to RabbitMQ")
              break
          except Exception as e:
              self.logger.error(f"Failed to connect to RabbitMQ: {e}")
              attempts += 1
              await asyncio.sleep(RABBITMQ_BACKOFF_FACTOR * attempts)
\end{lstlisting}

\subsection{Worker Allocation via gRPC}
The Live Processing Service requests worker allocation from the Worker Manager using gRPC. This ensures low-latency, efficient communication, which is critical for real-time transcription.

To allocate a worker for a transcription task, the Live Processing Service sends a gRPC request specifying the required ASR model. The Worker Manager responds with an available worker instance. This interaction leverages Protocol Buffers for efficient data serialization and the gRPC stub generated from the service definition.

Code \ref{lst:grpc_request} illustrates the gRPC request process.

\begin{lstlisting}[language=python, caption={gRPC Request for Worker Allocation}, label={lst:grpc_request}]
async def allocate_worker_from_worker_manger(self, model_name, task_queue):
      """
      Allocate an idle worker from the WorkerManager service.

      Args:
          model_name (str): Name of the model to allocate the worker for.
          task_queue (str): Name of the task queue to assign to the worker.
      """
      async with grpc.aio.insecure_channel(
          f"{WORKER_MANAGER_SERVICE}:{WORKER_MANAGER_PORT}"
      ) as channel:
          stub = worker_manager_pb2_grpc.WorkerManagerServiceStub(channel)
          request = worker_manager_pb2.AllocateWorkerRequest(
              model_name=model_name, task_queue=task_queue
          )
          response = await stub.AllocateWorker(request)

          if response.success:
              logger.info(
                  f"Worker {response.worker_name} allocated successfully: {response.message}"
              )
              return response.worker_name
          else:
              logger.error(f"Failed to allocate worker: {response.message}")
              return None
\end{lstlisting}


\section{Worker}
The primary function of the Worker is to process audio data using the specified ASR model and return transcription results.  

Each Worker loads an ASR model developed by the NTU Speech Lab, which is based on the Kaldi ASR toolkit \cite{kaldi}. The model is loaded into memory at startup, enabling low-latency transcription of incoming audio data.

\subsection{Audio Processing}
Once the Worker Manager assigns an idle Worker to a task via gRPC, the Worker retrieves the audio data from the task queue in RabbitMQ. It then processes the audio using the ASR model and publishes the transcription results to the result queue in RabbitMQ.  

To facilitate fault tolerance, the Worker updates its state in Redis when processing a task. It marks itself as \texttt{busy} and records the assigned task. This allows the Worker Manager to track active tasks and reassign them if a Worker fails.

Figure \ref{fig:worker_processing} illustrates the flow of audio processing in the Worker.
\begin{figure}[ht]
  \centering
  \includegraphics[width=.8\textwidth]{figures/worker_processing.drawio.png}
  \caption{Worker Processing Flow}
  \label{fig:worker_processing}
\end{figure}

\subsection{Heartbeat Monitoring}
To enable health monitoring, the Worker sends periodic heartbeat signals to the Worker Manager (Figure \ref{fig:worker_heartbeat}). These heartbeats allow the Worker Manager to detect failures and reallocate tasks accordingly. Further details on this mechanism are provided in Subsection \ref{subsection:worker_manager_monitor}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=.8\textwidth]{figures/worker_heartbeat.drawio.png}
  \caption{Worker Heartbeat Monitoring}
  \label{fig:worker_heartbeat}
\end{figure}

\section{Worker Manager} \label{section:worker_manager}
The Worker Manager is responsible for managing and coordinating worker pods to ensure efficient task processing and fault tolerance. Its core responsibilities include:
\begin{itemize}
  \item \textbf{Worker Allocation:} Assigning idle workers to process incoming audio data.
  \item \textbf{Health Monitoring:} Tracking worker health through periodic heartbeats.
  \item \textbf{Task Reassignment:} Detecting worker failures and reallocating tasks to maintain continuity.
  \item \textbf{Scaling Policy:} Dynamically adjusting the number of worker pods based on system load.
\end{itemize}


The Worker Manager consists of two primary components:
\begin{enumerate}
    \item \textbf{Worker Manager Server:} Exposes gRPC APIs for worker allocation and heartbeat monitoring.
    \item \textbf{Worker Manager Monitor:} Detects worker failures, reallocates tasks, and enforces scaling policies.
\end{enumerate}

\subsection{Worker Manager Server}

The Worker Manager Server provides two main gRPC endpoints (Code \ref{lst:worker_manager_server}):
\begin{enumerate}
    \item \texttt{AllocateWorker}: Assigns an idle worker to handle a given transcription task.
    \item \texttt{SendHeartbeat}: Receives periodic heartbeats from workers to track their health status. Stores the heartbeat timestamps in Redis for monitoring.
\end{enumerate}

Figure \ref{fig:worker_manager_server} shows how the Worker Manager Server interacts with the Live Processing Service, busy workers and Redis.

\begin{enumerate}
  \item [1] The Live Processing Service requests worker allocation via \texttt{AllocateWorker}.
  \item [2.1] The busy workers send periodic heartbeats to the Worker Manager Server via \texttt{SendHeartbeat}.
  \item [2.2] The Worker Manager Server then stores these heartbeat timestamps in Redis for monitoring.
\end{enumerate}

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{figures/worker_manager_server.drawio.png}
  \caption{Worker Manager Server Interaction}
  \label{fig:worker_manager_server}
\end{figure}


\subsection{Worker Manager Monitor} \label{subsection:worker_manager_monitor}
The Worker Manager Monitor is responsible for tracking worker health and handling task reallocation. It retrieves the latest heartbeat timestamps of workers from Redis. If a worker fails to send a heartbeat within a predefined timeout, it is considered unavailable, and its task is reassigned to another worker. This process is illustrated in Figure \ref{fig:worker_manager_monitor}.
\begin{figure}[ht]
  \centering
  \includegraphics[width=.8\textwidth]{figures/worker_manager_heartbeat.drawio.png}
  \caption{Worker Manager Heartbeat Monitoring}
  \label{fig:worker_manager_monitor}
\end{figure}

Code \ref{lst:worker_fault_tolerance} shows the snippet of the Worker Manager Monitor's fault tolerance mechanism for the workers.

\begin{lstlisting}[language=python, caption={Worker Fault Tolerance Mechanism}, label={lst:worker_fault_tolerance}]
  async def monitor_heartbeats(self):
        """
        Monitor worker heartbeats and reallocate workers if necessary.
        """
        logger.info("Monitoring worker heartbeats...")
        self.redis_client = self.redis.get_client()
        while True:
            current_time = asyncio.get_event_loop().time()
            # logger.debug(f"Current time: {current_time}")
            for worker_name, value in self.redis_client.hgetall(
                "WorkerHeartbeats"
            ).items():
                lock_key = f"lock:worker_heartbeat:{worker_name}"
                try:
                    # Acquire distributed lock
                    if self.acquire_lock(lock_key):
                        model_name, last_heartbeat = value.split(",")
                        if (
                            current_time - float(last_heartbeat)
                            > WORKER_HEARTBEAT_TIMEOUT
                        ):
                            logger.info(
                                f"Worker {worker_name} missed heartbeat. Allocating new worker."
                            )
                            try:
                                task_queue = self.redis_client.hget(
                                    "TaskAllocation", f"Worker:{worker_name}"
                                )
                            except Exception as e:
                                logger.error(f"Failed to get task queue: {str(e)}")
                                raise
                            self.allocate_worker(model_name, task_queue)
                            self.redis_client.hdel("WorkerHeartbeats", worker_name)
                            logger.info(f"Worker {worker_name} deallocated.")
                    else:
                        logger.warning(f"Failed to acquire lock for {worker_name}")
                except Exception as e:
                    logger.error(
                        f"Failed to monitor heartbeat for {worker_name}: {str(e)}"
                    )
                finally:
                    # Release distributed lock
                    self.release_lock(lock_key)

            await asyncio.sleep(WORKER_MANAGER_MONITOR_INTERVAL)
\end{lstlisting}

To prevent race conditions when multiple Worker Manager pods are deployed, the Worker Manager first acquires a distributed lock before checking a workerâ€™s last heartbeat. If a worker exceeds the timeout threshold, it is removed, and a new worker is assigned to its task queue.

\subsubsection{Testing Worker Fault Tolerance}
To validate the fault tolerance mechanism, the Worker Manager was tested under failure scenarios, such as unexpected worker crashes.

To simulate a worker failure, worker pod processing the audio data can be delibrately terminated using the following command:
\begin{verbatim}
  kubectl delete -n asr-sdk --now <worker-pod-name> \
    --cascade=background
\end{verbatim}

Figure \ref{fig:worker_fault_tolerance} illustrates the Worker Manager detecting that the worker pod \texttt{worker-\allowbreak stateful-english-0} has failed to send a heartbeat. As a result, it reallocates the task to an available idle worker, \texttt{worker-stateful-english-1}, ensuring uninterrupted processing.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/worker_fault_tolerance.png}
  \caption{Worker Fault Tolerance Testing}
  \label{fig:worker_fault_tolerance}
\end{figure}

\subsubsection{Worker Scaling Policy}
The Worker Manager dynamically scales worker pods based on system load. A configurable threshold, \texttt{SCALING\_TARGET}, defines the minimum number of idle pods required at any given time. The Worker Manager periodically evaluates worker utilization and adjusts the number of replicas accordingly to ensure optimal resource allocation.

Figure \ref{fig:worker_scaling_policy} illustrates an example where there are 2 busy worker pods and 2 idle worker pods. If the \texttt{SCALING\_TARGET} is set to 3, the Worker Manager scales up the Worker StatefulSet to 5 replicas when the number of idle pods falls below the threshold. Conversely, when the workers finish processing, the Worker Manager scales down the replicas to ensure the number of idle pods meets the \texttt{SCALING\_TARGET}.

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{figures/worker_scaling_policy.drawio.png}
  \caption{Worker Scaling Policy Example}
  \label{fig:worker_scaling_policy}
\end{figure}

Code \ref{lst:worker_scaling} shows the snippet of the Worker scaling mechanism.

\begin{lstlisting}[language=python, caption={Worker Scaling Policy}, label={lst:worker_scaling}]
async def monitor_and_scale():
    """
    Monitor the number of busy pods for each model and scale the statefulset up or down based on the number of idle pods.
    """

    while True:
        for model in MODELS:
            total_pods, busy_pods = get_pod_states(model)
            idle_pods = total_pods - busy_pods
            logger.info(
                f"Idle pods: {idle_pods}, Busy pods: {busy_pods}, Total pods: {total_pods}"
            )

            if idle_pods < SCALING_TARGET:
                # Scale up
                new_replicas = total_pods + (SCALING_TARGET - idle_pods)
                logger.info(f"Scaling up to {new_replicas} replicas")
                scale_statefulset(new_replicas)
            elif idle_pods > SCALING_TARGET:
                # Scale down
                new_replicas = total_pods - (idle_pods - SCALING_TARGET)
                logger.info(f"Scaling down to {new_replicas} replicas")
                scale_statefulset(new_replicas)

        # Wait for the next check
        await asyncio.sleep(CHECK_INTERVAL)
\end{lstlisting}

The system checks idle pods every \texttt{CHECK\_INTERVAL} seconds, ensuring that scaling adjustments do not occur too frequently. This is to allow for the previous scaling operation to taken effect before the next check, preventing excessive scaling activity.

\subsubsection{Testing Worker Scaling}
To validate the worker scaling policy, the system was tested under varying workloads to observe how the Worker Manager adapts in real time.

Figure \ref{fig:worker_scaling_up_down} shows the logs of the Worker Manager, illustrating the dynamic scaling behavior of the Worker Manager. When a transcription task starts and an available worker is assigned to process it, the number of idle pods drops below the \texttt{SCALING\_TARGET}, triggering an automatic scale-up. Conversely, once the task is completed and the worker becomes idle again, the Worker Manager scales down the replicas to optimize resource usage.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/worker_scaling_up_down.png}
  \caption{Worker Scaling Up and Down}
  \label{fig:worker_scaling_up_down}
\end{figure}

\subsubsection{Health and Readiness Checks}
The Worker Manager Monitor performs health and readiness checks to ensure it is ready to handle requests. These checks are essential for maintaining the reliability and availability of the system. 

Code \ref{lst:health_check} shows the implementation of the health and readiness checks.

\begin{lstlisting}[language=python, caption={Worker Manager Monitor Health and Readiness Checks}, label={lst:health_check}]
async def healthz(request):
    return web.Response(text="OK")

async def ready(request):
    # Check Redis connection
    try:
        redis_client = WorkerManagerRedisClient().get_client()
        redis_client.ping()
    except Exception as e:
        logger.error(f"Readiness check failed: Redis connection error: {str(e)}")
        return web.Response(status=500, text="Redis connection error")

    # Check if the monitor can acquire a lock
    lock_key = "readiness_check_lock"
    if not monitor.acquire_lock(lock_key, timeout=5):
        logger.error("Readiness check failed: Unable to acquire lock")
        return web.Response(status=500, text="Unable to acquire lock")
    monitor.release_lock(lock_key)

    return web.Response(text="OK")
\end{lstlisting}

The health check (\texttt{healthz}) provides a basic indicator of whether the Worker Manager Monitor is running. It responds with an "OK" message to confirm the service is active.
The readiness check (\texttt{ready}) performs the following validations to ensure the monitor is ready to function:
\begin{enumerate}
    \item \textbf{Redis Connection:} Verifies the monitor can connect to Redis by sending a \texttt{PING} request.
    \item \textbf{Lock Acquisition:} Ensures the monitor can acquire and release a distributed lock, which is critical for managing shared resources.
\end{enumerate}

When deployed on Kubernetes, it continuously monitors the readiness and liveness endpoints to determine if the service is ready to receive traffic or if it needs to be restarted. Figure \ref{fig:k8s_healthcheck} shows the successful readiness and liveness check at every 10 seconds interval.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/k8s_healthcheck.png}
  \caption{Kubernetes Readiness and Liveness Checks}
  \label{fig:k8s_healthcheck}
\end{figure}

\section{Infrastructure Deployment}
The infrastructure is deployed using Terraform, an Infrastructure-as-Code (IaC) tool that allows for the provisioning of cloud resources in a declarative manner. Terraform compares the desired state defined in configuration files with the current state of the cloud environment and makes the necessary changes to achieve the desired state.

\subsection{AWS Access Key and CLI Configuration}
To interact with AWS services, it is necessary to create an AWS access key. This can be done by navigating to the account credentials section in the AWS Management Console and creating an access key.

Once the access key is generated, the AWS CLI can be configured by running the command:
\begin{verbatim}
  aws configure
\end{verbatim}
The CLI will prompt for the access key, secret key, region, and output format. For ease of use, a named profile can also be set up using:
\begin{verbatim}
  aws configure --profile <profile-name>
\end{verbatim}
This enables switching between different AWS accounts and regions as needed.

\subsection{Setting Up Terraform Configuration}
Terraform is used to manage infrastructure as code. There are three main Terraform commands:
\begin{itemize}
    \item \texttt{terraform init}: Initializes the configuration, downloads necessary providers, and sets up modules.
    \item \texttt{terraform plan}: Generates an execution plan to show the changes Terraform will make to the infrastructure. Code \ref{lst:terraform_plan} shows an example of the Terraform execution plan output.
    \item \texttt{terraform apply}: Applies the changes to create or update infrastructure resources. This will update our cloud environment to match the desired state defined in the Terraform configuration files.
\end{itemize}

\subsection{Storing State Files in S3}
To enable collaboration among multiple developers, the Terraform state file can be stored in an Amazon S3 bucket. The state file contains the current infrastructure state and helps Terraform determine necessary changes for future deployments.


Code \ref{lst:state_bucket} shows the Terraform configuration for setting up the state bucket. We can set the bucket name and tags as variables to customize the bucket's name and attributes. This can be done by defining the variables in a separate \texttt{variables.tf} file.

\begin{lstlisting}[language=Terraform, caption={Terraform Configuration for Setting Up State Bucket}, label={lst:state_bucket}]
resource "aws_s3_bucket" "terraform_state" {
  bucket = var.bucket_name

  tags = var.tags
}

resource "aws_s3_bucket_versioning" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id
  versioning_configuration {
    status = "Enabled"
  }
}

resource "aws_s3_bucket_public_access_block" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
\end{lstlisting}

Amazon S3 is a highly durable, scalable, and secure object storage service provided by AWS \cite{s3}. By leveraging S3 for Terraform state file storage, teams can ensure that the state file is reliably stored and accessible across all environments. This approach also promotes collaboration, consistency, and secure infrastructure management.

To prevent race conditions when multiple developers modify the infrastructure simultaneously, a DynamoDB table \cite{dynamodb} is used to store the state lock. Code \ref{lst:dynamodb_table} shows the DynamoDB table configuration.

\begin{lstlisting}[language=Terraform, caption={Terraform Configuration for Setting Up DynamoDB Table}, label={lst:dynamodb_table}]
resource "aws_dynamodb_table" "terraform-lock" {
    name = "terraform-lock"
    hash_key = "LockID"
    read_capacity = 10
    write_capacity = 10
    
    attribute {
      name = "LockID"
      type = "S"
    }
    
    tags = {
      Name = "Terraform Lock Table"
    }
}
\end{lstlisting}

The Terraform backend configuration can then be updated to use the S3 bucket and DynamoDB table, as shown in Code~\ref{lst:backend_config}.

\begin{lstlisting}[language=Terraform, caption={Terraform Backend Configuration}, label={lst:backend_config}]
terraform {
    backend "s3" {
        bucket         = "terraform-state-bucket"
        key            = "terraform.tfstate"
        region         = "ap-southeast-1"
        dynamodb_table = "terraform-lock"
    }
}
\end{lstlisting}

\subsection{Deploying Terraform Resources}
The Terraform configuration files reference Song's Terraform modules \cite{song_yu}, with enhancements such as storing the Terraform state in an S3 bucket and simplifying deployment by creating an EFS CSI driver for the EFS volume. This driver allows Kubernetes pods to mount the EFS volume, enabling seamless integration.

The Terraform configuration for creating the EFS CSI driver and attaching it to the EFS volume is shown in Code \ref{lst:efs_csi_driver}. This configuration sets up the necessary IAM role, policy attachments, and the EFS CSI driver for an EKS cluster.

To deploy these resources, the following commands are executed:
\begin{itemize}
    \item \texttt{terraform init}: Initializes the Terraform configuration and downloads the necessary providers and modules.
    \item \texttt{terraform plan}: Generates an execution plan that shows the changes Terraform will make to the infrastructure.
    \item \texttt{terraform apply}: Applies the changes to the infrastructure and deploys the resources.
\end{itemize}

\subsection{Deploying EFS and Attaching to Models}
To store the ASR models, an Elastic File System (EFS) can be deployed. The following steps outline the deployment process:
\begin{enumerate}
    \item Create an EC2 instance in a public subnet.
    \item Attach the EFS volume to the instance.
    \item Copy the ASR model files to the EFS volume.
\end{enumerate}


Code \ref{lst:ec2_setup} shows the Terraform configuration for setting up an EC2 instance and an SSH key. 

After the EC2 instance is created, follow these steps to transfer the model files:

\begin{enumerate}
    \item Retrieve the EC2 public IP from the AWS console.
    \item SSH into the EC2 instance using \texttt{ssh -i <key.pem> ec2-user@<public-ip>}.
\end{enumerate}

On the EC2 instance:
\begin{enumerate}
    \item \texttt{sudo mkdir -p /mnt/efs}: Create a directory to mount the EFS volume.
    \item 
        \begin{sloppypar}
        \texttt{sudo mount -t nfs4 -o nfsvers=4.1 <mount-target address>:/ /mnt/efs}: Mount the EFS volume to the directory.
        \end{sloppypar}
    \item \texttt{sudo chmod go+rw /mnt/efs}: Update the permissions of the directory to allow read and write access.

    \item 
        \begin{sloppypar}
        \texttt{scp -r -i "<private key name>.pem"   \\<model-directory>@<public-ip>:/mnt/efs}: Copy the model files to the EFS volume.
        \end{sloppypar}
\end{enumerate}

\section{Kubernetes Cluster}
\texttt{kubectl} is the primary command-line tool for interacting with Kubernetes clusters. It allows users to create, update, and manage resources within the cluster. Below are some commonly used \texttt{kubectl} commands:

\begin{itemize}
    \item \texttt{kubectl apply -f <file>}: Applies the configuration defined in the YAML file to the cluster.
    \item \texttt{kubectl get <resource>}: Retrieves information about the specified resource.
    \item \texttt{kubectl describe <resource> <name>}: Provides detailed information about the specified resource.
    \item \texttt{kubectl logs <pod-name>}: Displays the logs of the specified pod.
    \item \texttt{kubectl exec -it <pod-name> -- /bin/bash}: Opens a shell in the specified pod.
    \item \texttt{kubectl delete <resource> <name>}: Deletes the specified resource.
\end{itemize}

To connect to the EKS cluster, the AWS CLI can be used to update the \texttt{kubeconfig} file with the cluster details. The following command retrieves the cluster configuration and updates the kubeconfig file:
\begin{verbatim}
aws eks --region <region> update-kubeconfig --name <cluster-name>
\end{verbatim}

Once the kubeconfig file is updated, kubectl can be used to interact with the EKS cluster.

\subsection{Deploying ASR System Components}
The ASR system can be deployed to the Kubernetes cluster using the Kubernetes manifest files provided in the repository. The deployment process can be initiated with the following command:
\begin{verbatim}
kubectl apply -f <file>
\end{verbatim}

The deployment includes the following components:
\begin{itemize}
    \item \textbf{Live Processing Service Deployment:} Deploys the Live Processing Service to manage transcription tasks.
    \item \textbf{Worker Manager Server and Monitor Deployment:} Deploys the Worker Manager Server to handle gRPC requests and the Worker Manager Monitor to manage worker health.
    \item \textbf{Worker Deployment:} Deploys the worker pods responsible for processing audio data.
    \item \textbf{Persistent Volume and Persistent Volume Claim:}  Defines the storage requirements for the EFS  volume used by the ASR system.
\end{itemize}

\subsection{Horizontal Pod Autoscaler (HPA)}
The Kubernetes Horizontal Pod Autoscaler (HPA) automatically adjusts the number of pods in a deployment based on resource utilization, such as CPU or memory, or custom metrics. This ensures efficient resource allocation and improved system responsiveness under varying workloads.

The HPA configuration is defined in a YAML file and applied to the cluster using:
\begin{verbatim}
  kubectl apply -f <file>
\end{verbatim}

Code \ref{lst:HPA_live_processing_server} provides an example HPA configuration for the Live Processing Server.


\begin{lstlisting}[language=Kubernetes, caption={Horizontal Pod Autoscaler for Live Processing Server}, label={lst:HPA_live_processing_server}]
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: live-processing-server-hpa
  namespace: asr-sdk
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: live-processing-server-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 75
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
\end{lstlisting}

This configuration defines the following:
\begin{itemize}
    \item \textbf{Target Deployment:} The HPA scales the \texttt{live-processing-server-deployment}.
    \item \textbf{Replica Scaling Range:} The deployment is maintained between a minimum of 2 and a maximum of 10 replicas.
    \item \textbf{Scaling Metrics:} The autoscaler monitors CPU and memory utilization, scaling up or down when usage exceeds or falls below 75\%.
\end{itemize}
\subsubsection{Testing the HPA}
To verify the HPA's behavior under load, a benchmarking tool such as \texttt{wrk} can be used to simulate traffic.

The following command generates a load test by sending 25 concurrent WebSocket requests for 30 seconds using 12 threads:
\begin{verbatim}
  wrk -t12 -c25 -d30s ws://<live-processing-service-url>/client/ws/speech
\end{verbatim}

To monitor the HPA scaling behavior in real-time, use:
\begin{verbatim}
  kubectl get hpa -n asr-sdk live-processing-server-hpa --watch
\end{verbatim}

As shown in Figure \ref{fig:hpa_cli}, after 30 seconds of sustained load, the CPU utilization surpasses the 75\% threshold, triggering the HPA to scale the deployment from 2 to 7 replicas.

\begin{figure}[!h]
  \centering
  \includegraphics[width=\textwidth]{figures/hpa_cli.png}
  \caption{Testing of HPA with wrk}
  \label{fig:hpa_cli}
\end{figure}

\subsection{Helm Charts}
Helm charts were used to deploy the RabbitMQ and Redis clusters. The charts used were from Bitnami, which provides pre-configured Helm charts for popular applications, including RabbitMQ and Redis.

Users can customize their deployments by specifying configuration parameters in a \texttt{values.yaml} file. This file allows for modifications such as the number of replicas, resource limits, and persistence settings. The full list of configurable values is available in the official Helm chart repository.


The following commands deploy the Redis and RabbitMQ clusters using Helm:
\begin{verbatim}
  # Deploy Redis cluster
  helm install redis oci://registry-1.docker.io/bitnamicharts/redis \
    -f values.yaml --namespace asr-sdk --create-namespace
  
  # Deploy RabbitMQ cluster
  helm install rabbitmq oci://registry-1.docker.io/bitnamicharts/rabbitmq \
    -f values.yaml --namespace asr-sdk --create-namespace
\end{verbatim}

To ensure data persistence for RabbitMQ and Redis, Amazon Elastic Block Store (EBS) can be configured as the persistent storage backend.
\begin{enumerate}
  \item Install the EBS CSI driver on the EKS cluster if it is not already installed.
  \item Annotate the EBS CSI service account with the IAM role created for EBS via Terraform using the following command:
  \begin{verbatim} 
  kubectl annotate serviceaccount ebs-csi-controller-sa -n kube-system \
    eks.amazonaws.com/role-arn=<ARN_of_EBS_CSI_Driver_Role>
  \end{verbatim}
\end{enumerate}

\subsection{Kubernetes Dashboard}
To effectively visualize the Kubernetes cluster and monitor deployed resources, the Kubernetes Dashboard provides a graphical interface for managing the cluster, inspecting workloads, and troubleshooting issues. The dashboard allows users to view real-time metrics, manage deployments, and analyze resource utilization. Figure \ref{fig:kubernetes_dashboard} illustrates an example of the Kubernetes Dashboard.
\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/kubernetes_dashboard.png}
  \caption{Kubernetes Dashboard}
  \label{fig:kubernetes_dashboard}
\end{figure}

\subsubsection{Installing the Kubernetes Dashboard}
The Kubernetes Dashboard can be installed using Helm with the following commands:

\begin{verbatim}
  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
  helm upgrade --install kubernetes-dashboard \
    kubernetes-dashboard/kubernetes-dashboard \
    --create-namespace --namespace kubernetes-dashboard
\end{verbatim}

\subsubsection{Configuring Access and Permissions}
Before accessing the dashboard, a set of required resources must be configured, including:

\begin{itemize}
  \item \textbf{Service Account:} Grants access to the dashboard.
  \item \textbf{Cluster Role Binding:} Assigns permissions to the service account.
  \item \textbf{Secret Token:} Authenticates the dashboard.
\end{itemize}

The Kubernetes configuration file in Code \ref{lst:k8s_dashboard_dependencies} sets up these dependencies.


\subsubsection{Accessing the Dashboard}
Once the dashboard and necessary permissions are set up, port forwarding can be used to expose the dashboard locally:
\begin{verbatim}
  kubectl -n kubernetes-dashboard port-forward \
    svc/kubernetes-dashboard-kong-proxy 8443:443
\end{verbatim}

After running this command, open a web browser and navigate to: \texttt{https://localhost:8443}.

\subsubsection{Generating Token}
To log in to the dashboard, generate an authentication token using:
\begin{verbatim}
  kubectl -n kubernetes-dashboard create token admin-user
\end{verbatim}
