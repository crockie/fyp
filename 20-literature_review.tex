\chapter{Literature Review} \label{chapter:literature_review}
The development of reliable and scalable Automatic Speech Recognition (ASR) systems requires an understanding of modern distributed systems technologies and architectural design. This literature review examines the technologies and approaches relevant to enhancing ASR system scalability and resilience. The review begins with previous work on the system, followed by an analysis of key technologies in distributed systems, containerization, message queues, cloud infrastructure, and chaos engineering.

\section{Previous Work}
Putra \cite{putra} had worked on the same ASR system and his project effectively highlights the advantages of transitioning from a tightly coupled ASR system to a decoupled microservices architecture using Apache Kafka. The use of Kubernetes and other modern cloud-native tools such as Kyverno, Falco, and Knative demonstrates a robust effort to address scalability, reliability, and security challenges in ASR systems. The project discusses the integration of Kafka as the message broker to decouple the master and worker components, ensuring fault tolerance and enabling the system to recover from worker crashes without losing data.

\subsection{Research Gap}
A significant research gap exists in the implementation's choice of message broker technology. The selection of Kafka for the ASR system raises concerns regarding its fit for scenarios requiring high data accuracy and context preservation. While Kafka's high throughput and durability are valuable for many systems, ASR workflows are fundamentally bottlenecked by the processing speed of workers, not the message broker. This mismatch between technology choice and system requirements suggests that RabbitMQ would be a more suitable alternative due to its task-oriented features and built-in support for state-dependent processing, which better aligns with the sequential nature of speech processing tasks.

Furthermore, while Putraâ€™s work \cite{putra} demonstrated promising results, a practical limitation emerged as the research team no longer has access to his project's codebase. This circumstance has created an opportunity to revisit the system's architecture with fresh perspective, particularly in the areas of component decoupling and scaling mechanisms. The current project therefore aims to not only address the technological fit of the message broker but also to establish a well-documented implementation that can be maintained and evolved by the research team.


\section{Distributed Systems Architecture}
\subsection{Evolution of Microservices}
The transition from monolithic to microservices architecture represents a fundamental shift in distributed systems design. Newman \cite{newman} defines microservices as small, autonomous services that work together, focusing on modularity and independent deployability. This architectural style has gained prominence due to its ability to support scalability, maintainability, and team autonomy \cite{microservices_benefits}. In the context of ASR systems, microservices architecture enables independent scaling of components and improved fault isolation.

\subsection{gRPC in Modern Applications}
gRPC is a high-performance Remote Procedure Call (RPC) \cite{grpc}, which led to a significant advancement in service-to-service communication. Niswar et al. \cite{grpc_comparison} conducted performance analyses showing that gRPC outperforms REST APIs and GraphQL in terms of response time for fetching both flat and nested data, as well as CPU utilisation. It utilises Protocol Buffers which provide a language-agnostic interface definition \cite{protocol_buffers}, allowing services written in different programming languages to communicate efficiently. This is crucial in microservices architectures where different teams might develop services using different technologies. These features make gRPC particularly suitable for ASR systems where reliable, high-performance communication between components is essential.

\section{Containerization}
\subsection{Docker}
Docker is a service that leverages on operating system level virtualisation to package software into containers \cite{docker_definition}. Bernstein \cite{Bernstein} explains how Docker containers package applications with their dependencies. This consistency is crucial for ASR systems, where complex model and service dependencies must be managed effectively.

\subsection{Kubernetes}
Kubernetes is a container orchestration tool used to automate the deployment and management of containers \cite{k8s_definition}. Burns et al. \cite{k8s_architecture} detail its architecture and ability to manage containerized applications at scale. For ASR systems, Kubernetes provides essential features such as automatic scaling, self-healing, and rolling updates \cite{k8s_features}, which are crucial for maintaining service reliability.

\subsection{Helm}
Helm is a package manager for Kubernetes applications \cite{helm_definition}. Helm utilises Charts, as reusable packages that contains pre-configured Kubernetes resources \cite{helm_charts}, making complex application deployments more manageable. These Charts function as templates that can be customized through value files \cite{helm_definition}, enabling environment-specific configurations while maintaining consistency in the underlying architecture.

\section{Amazon Web Services (AWS)}
AWS is a cloud service provider that offers a wide range of products and services. These services span compute, storage, networking, database, and container management, among others \cite{aws_definition}.

\subsection{Virtual Private Cloud (VPC)}
Amazon VPC forms the networking foundation for AWS resources, providing an isolated virtual network environment in the cloud \cite{vpc}. It enables users to define network architecture with custom IP address ranges, subnets, and routing tables \cite{vpc}. A key feature is its ability to span multiple Availability Zones (AZs) within a region, enhancing system resilience through geographical distribution \cite{vpc_az}.

\subsection{Elastic Compute Cloud (EC2)}
Amazon EC2 is a computing service that provides scalable virtual machines (instances) in the cloud \cite{ec2_definition}. It offers a wide range of instance types optimized for different use cases, from compute-intensive applications to memory-intensive workloads \cite{ec2_instance_types}. EC2 instances can be launched across multiple AZs for high availability. EC2 pricing models including on-demand, reserved, and spot instances to optimize costs based on workload patterns \cite{ec2_pricing}.

\subsection{Identity and Access Management (IAM)}
IAM provides fine-grained access control to AWS resources \cite{iam_definition}. It implements the principle of least privilege through a comprehensive policy framework that defines who (principal) can do what (actions) on which resources under specific conditions \cite{iam_security}. IAM enables organizations to manage user identities, roles, and permissions centrally, ensuring secure access to cloud resources while maintaining compliance requirements.

\subsection{Elastic Kubernetes Service (EKS)}
Amazon EKS is a managed Kubernetes service that simplifies the deployment, management, and scaling of containerized applications \cite{eks_definition}. It automatically manages the availability and scalability of the Kubernetes control plane across multiple AZs \cite{eks_definition}. EKS integrates seamlessly with other AWS services and supports various deployment models, including hybrid architectures that span cloud and on-premises environments \cite{eks_deployment}.

\subsection{Elastic Container Registry (ECR)}
Amazon ECR is a managed container registry service that simplifies the storage, management, and deployment of container images \cite{ecr_definition}. It provides encrypted image storage and integrates with AWS IAM for access control \cite{ecr_iam}. ECR features automatic image scanning for vulnerabilities \cite{ecr_image_scanning} and lifecycle policies for image management \cite{ecr_lifecycle}, making it an essential component in container-based architectures.

\subsection{Elastic File System (EFS)}
Amazon EFS provides scalable, fully managed network file storage for use with AWS cloud services and on-premises resources \cite{efs_definition}. Supporting the Network File System version 4 (NFSv4) protocol \cite{efs_work}, EFS can be accessed concurrently by thousands of compute instances \cite{efs_performance}. It automatically scales throughput when files are added or removed \cite{efs_performance}, making it ideal for applications requiring shared file access across multiple instances or containers.

\section{Infrastructure-as-Code (IaC)}
IaC is an approach to managing and provisioning computing infrastructure through  configuration files, rather than through physical hardware configuration or interactive configuration tools. This method allows for the automation of infrastructure setup, ensuring consistency and reducing the risk of human error \cite{iac_benefits}.

\subsection{Terraform}
Terraform is an IaC tool that allows for the declarative management of cloud resources \cite{terraform_hashicorp}. This means that we define the desired final state of our architecture, and Terraform will apply changes only when necessary to achieve that state \cite{terraform_declarative}. Unlike traditional manual deployment through cloud provider consoles (often referred to as "ClickOps" \cite{clickops}), Terraform enables organizations to define their infrastructure using declarative configuration files. This code-driven approach transforms infrastructure deployment from a manual, error-prone process into an automated, version-controlled workflow.

Terraform enables teams to consistently replicate environments across development, testing, and production stages \cite{iac_benefits}, ensuring that infrastructure configurations remain identical at each phase. By maintaining infrastructure as code, teams can version control their changes, enabling peer reviews and the ability to roll back modifications if issues arise. Terraform also manages dependencies between different cloud resources automatically \cite{terraform_dependencies}, reducing the complexity of infrastructure deployment. 

\section{In-memory Data Storage}
\subsection{Redis}
Redis is a high-performance, in-memory data store commonly used as a cache and a key-value database \cite{redis_definition}. Redis is fast, and thus well-suited for use in ASR systems, where it can store session information and temporary transcription data. This enables rapid access to frequently used data and facilitates reliable state management, ensuring smooth and responsive system performance.

\section{Message Queues}
Message queues enable asynchronous communication between services in distributed systems, providing temporary message storage and reliable delivery mechanisms \cite{queue_definition}. This asynchronous pattern facilitates decoupling of producers and consumers \cite{queue_decouple}, allowing components to scale independently and operate without direct dependencies on each other. 

\subsection{Apache Kafka}
Apache Kafka is designed as a distributed log-based messaging system, and its main use case is for ingesting and streaming real-time data \cite{kafka_definition}. Kafka's architecture centers around append-only logs (topics) divided into partitions, where messages are immutably stored and accessed via offset-based positioning \cite{kafka_documentation}.

\subsection{RabbitMQ}
RabbitMQ implements the Advanced Message Queuing Protocol (AMQP) \cite{rabbitmq_protocols} and operates as a broker-based message queue \cite{rabbitmq_definition}. It provides sophisticated message routing capabilities through exchanges and queues, supporting various patterns including publish-subscribe, and request-reply communications \cite{rabbitmq_routing}. RabbitMQ offers features such as message acknowledgments, dead letter queues, and priority queuing \cite{rabbitmq_routing}, making it particularly suitable for complex message routing requirements.

\subsection{Comparison of Kafka and RabbitMQ}
While both systems are robust message brokers, their architectural differences significantly impact their suitability for ASR applications. Below, we compare Kafka and RabbitMQ across key dimensions relevant to ASR systems.

\subsubsection{Message Ordering}
Kafka's partitioned architecture, while enabling high throughput, cannot guarantee message ordering across partitions. Dobbelaere and Esmaili \cite{kafka_v_rabbitmq} note that Kafka's ordering guarantees are limited to individual partitions.

RabbitMQ, through its queue-based architecture, maintains strict FIFO (First-In-First-Out) ordering within queues in the same channel \cite{kafka_v_rabbitmq}, making it more suitable for ASR systems where speech context and sequence are crucial.

\subsubsection{Message Delivery Guarantees}
Kafka provides at-least-once delivery semantics through offset management \cite{kafka_v_rabbitmq}, but consumers must handle offset commits carefully to avoid message reprocessing.

RabbitMQ's acknowledgment mechanism offers more flexible delivery guarantees, with built-in support for message acknowledgment \cite{kafka_v_rabbitmq} and automatic requeuing of unprocessed messages \cite{rabbitmq_nack}.

\subsubsection{Processing Requirements}
For ASR systems, where maintaining speech context is paramount \cite{speech_context}, RabbitMQ's single-queue consumer model aligns better with the need for sequential processing.

The research shows that while Kafka's throughput advantage is significant for high-volume streaming \cite{kafka_v_rabbitmq}, this benefit is less relevant for ASR workloads where processing speed is typically bounded by the speech recognition models rather than message throughput.

\subsubsection{Message Queue Selection}
Based on these considerations and supported by Dobbelaere and Esmaili's \cite{kafka_v_rabbitmq} findings, RabbitMQ emerges as the more appropriate choice for ASR systems due to its strong message ordering guarantees, flexible acknowledgement mechanisms, and support for sequential processing requirements. By leveraging RabbitMQ's features, ASR systems can ensure accurate transcription results and maintain the context of speech data throughout the processing pipeline.

\section{Chaos Engineering}
Chaos engineering is an approach to deliberately introducing failures to identify weaknesses and improve resilience \cite{chaos_engineering_definition}. By simulating conditions like Kubernetes pod failure, network delay, and node stress \cite{chaos_mesh_feature}, chaos engineering helps us observe system behaviour under stress and improve system robustness \cite{chaos_engineering_definition}. It also enhances incident response time by enabling a better understanding of failure scenarios \cite{chaos_engineering_definition}. Some chaos engineering tools include Chaos Mesh \cite{chaos_mesh_introduction} and AWS Fault Injection Simulator (FIS) \cite{fis_introduction}.

